{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3269cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jsonl file\n",
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"train.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "print(train[0]['question'], train[0]['points'], train[0]['article'])\n",
    "print(len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92760bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate proportion of testcases where answer can be found in the question\n",
    "total = 0\n",
    "found = 0\n",
    "for set in train:\n",
    "    total += 1\n",
    "    if set['article'].lower() in set['question'].lower():\n",
    "        found += 1\n",
    "\n",
    "print(found,total,found/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not as many as I thought, but still a significant amount\n",
    "\n",
    "# Perhaps the points are a good indicator of more trivial questions, where the answer is in the question?\n",
    "# Investigate the distribution of points\n",
    "\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trivial = []\n",
    "nontrivial = []\n",
    "\n",
    "for set in train:\n",
    "    if set['article'].lower() in set['question'].lower():\n",
    "        trivial.append(set['points'])\n",
    "    else:\n",
    "        nontrivial.append(set['points'])\n",
    "\n",
    "all = trivial + nontrivial\n",
    "\n",
    "print(mean(trivial), mean(nontrivial))\n",
    "print(mean(all))\n",
    "\n",
    "sns.displot(trivial)\n",
    "sns.displot(nontrivial)\n",
    "plt.show()\n",
    "\n",
    "# There is a strong correlation, perhaps we can bias the model towards \n",
    "# retrieving the answer from the question if the points are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I suspect most of the answers are nouns. Let's use NLP to check this.\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "is_noun = 0\n",
    "not_noun = 0\n",
    "not_noun_examples = []\n",
    "\n",
    "docs = list(nlp.pipe([set['article'] for set in train]))\n",
    "for doc in docs:\n",
    "    if len(list(doc.noun_chunks)) >= 1:\n",
    "        is_noun += 1\n",
    "    else:\n",
    "        not_noun += 1\n",
    "        not_noun_examples.append(doc.text)\n",
    "\n",
    "print(is_noun, not_noun, is_noun/(is_noun+not_noun))\n",
    "print(not_noun_examples[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than 85% of the articles are nouns, so we should prioritise nouns in our search.\n",
    "# Many of the articles not classified as nouns are in fact nouns, many of them being years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's process the wikipedia dataset using parquet\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10210253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "wikipedia = pq.read_table('train-00000-of-00001.parquet').to_pandas()\n",
    "wikipedia = wikipedia[:10000]\n",
    "wikipedia = wikipedia[['text', 'title']]\n",
    "print(wikipedia.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best way to tackle this problem should be to use a vector database. Let's set up milvus for this.\n",
    "# Milvus is being run in a docker container in the milvus folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to milvus server\n",
    "# Credit to this tutorial by Stephen Collins for information on setting up milvus and text embedding\n",
    "# https://dev.to/stephenc222/how-to-use-milvus-to-store-and-query-vector-embeddings-5hhl\n",
    "from pymilvus import connections\n",
    "\n",
    "def connect_to_milvus():\n",
    "    try:\n",
    "        connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "        print(\"Connected to Milvus.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Milvus: {e}\")\n",
    "        raise\n",
    "\n",
    "connect_to_milvus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c52ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up schema and create a collection\n",
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "def create_collection(name, fields, description):\n",
    "    schema = CollectionSchema(fields, description)\n",
    "    collection = Collection(name, schema, consistency_level=\"Strong\")\n",
    "    return collection\n",
    "\n",
    "def drop_collection(name):\n",
    "    collection = Collection(name)\n",
    "    collection.drop()\n",
    "    \n",
    "# Define fields for our collection\n",
    "fields = [\n",
    "    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n",
    "    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=500),\n",
    "]\n",
    "\n",
    "drop_collection(\"wikipedia_simple\")\n",
    "collection = create_collection(\"wikipedia_simple\", fields, \"Text embeddings of the simple wikipedia dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb65455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_util import generate_embeddings\n",
    "# Generate embeddings for each article\n",
    "for i, doc in enumerate(wikipedia['text']):\n",
    "    embedding = generate_embeddings(doc)\n",
    "    # Write into file\n",
    "    with open(\"embeddings.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(f\"{embedding}\\n\")\n",
    "    print(f\"{i}/{len(wikipedia)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embeddings\n",
    "with open(\"embeddings.txt\", \"r\", encoding='utf-8') as f:\n",
    "    embeddings = f.readlines()\n",
    "\n",
    "embeddings = [[float(value) for value in embedding[1:-2].split(\", \")] for embedding in embeddings]\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into milvus\n",
    "entities = [\n",
    "    [str(i) for i in range(len(wikipedia))],\n",
    "    embeddings,\n",
    "    [str(title) for title in wikipedia['title']],\n",
    "]\n",
    "\n",
    "insert_result = collection.insert(entities)\n",
    "print(insert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for embeddings\n",
    "def create_index(collection, field_name, index_type, metric_type, params):\n",
    "    index = {\"index_type\": index_type, \"metric_type\": metric_type, \"params\": params}\n",
    "    collection.create_index(field_name, index)\n",
    "\n",
    "create_index(collection, \"embeddings\", \"IVF_FLAT\", \"L2\", {\"nlist\": 128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca80661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_query(collection, search_vectors, search_field, search_params):\n",
    "    collection.load()\n",
    "    result = collection.search(search_vectors, search_field, search_params, limit=3, output_fields=[\"title\"])\n",
    "    return result[0][0].entity.get(\"title\")\n",
    "\n",
    "# Test search\n",
    "query = \"how do living organisms in a natural environment respond to changes in weather or climate?\"\n",
    "query_vector = generate_embeddings(query)\n",
    "search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}})\n",
    "\n",
    "# Correctly returns \"Environment\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of our model\n",
    "score = 0\n",
    "totalScore = 0\n",
    "\n",
    "for set in train:\n",
    "    query = set['question']\n",
    "    query_vector = generate_embeddings(query)\n",
    "    result = search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}})\n",
    "    if result.lower() is set['article'].lower():\n",
    "        score += set['points']\n",
    "    totalScore += set['points']\n",
    "\n",
    "print(f\"Our model scored {score}/{totalScore} points on the training set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

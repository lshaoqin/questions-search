{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3269cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jsonl file\n",
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"train.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "print(train[0]['question'], train[0]['points'], train[0]['article'])\n",
    "print(len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92760bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate proportion of testcases where answer can be found in the question\n",
    "total = 0\n",
    "found = 0\n",
    "for set in train:\n",
    "    total += 1\n",
    "    if set['article'].lower() in set['question'].lower():\n",
    "        found += 1\n",
    "\n",
    "print(found,total,found/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not as many as I thought, but still a significant amount\n",
    "\n",
    "# Perhaps the points are a good indicator of more trivial questions, where the answer is in the question?\n",
    "# Investigate the distribution of points\n",
    "\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trivial = []\n",
    "nontrivial = []\n",
    "\n",
    "for set in train:\n",
    "    if set['article'].lower() in set['question'].lower():\n",
    "        trivial.append(set['points'])\n",
    "    else:\n",
    "        nontrivial.append(set['points'])\n",
    "\n",
    "all = trivial + nontrivial\n",
    "\n",
    "print(mean(trivial), mean(nontrivial))\n",
    "print(mean(all))\n",
    "\n",
    "sns.displot(trivial)\n",
    "sns.displot(nontrivial)\n",
    "plt.show()\n",
    "\n",
    "# There is a strong correlation, perhaps we can bias the model towards \n",
    "# retrieving the answer from the question if the points are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I suspect most of the answers are nouns. Let's use NLP to check this.\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "is_noun = 0\n",
    "not_noun = 0\n",
    "not_noun_examples = []\n",
    "\n",
    "docs = list(nlp.pipe([set['article'] for set in train]))\n",
    "for doc in docs:\n",
    "    if len(list(doc.noun_chunks)) >= 1:\n",
    "        is_noun += 1\n",
    "    else:\n",
    "        not_noun += 1\n",
    "        not_noun_examples.append(doc.text)\n",
    "\n",
    "print(is_noun, not_noun, is_noun/(is_noun+not_noun))\n",
    "print(not_noun_examples[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than 85% of the articles are nouns, so we should prioritise nouns in our search.\n",
    "# Many of the articles not classified as nouns are in fact nouns, many of them being years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's process the wikipedia dataset using parquet\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10210253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "wikipedia = pq.read_table('train-00000-of-00001.parquet').to_pandas()\n",
    "wikipedia = wikipedia[:10000]\n",
    "wikipedia = wikipedia[['text', 'title']]\n",
    "print(wikipedia.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best way to tackle this problem should be to use a vector database. Let's set up milvus for this.\n",
    "# Milvus is being run in a docker container in the milvus folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to milvus server\n",
    "# Credit to this tutorial by Stephen Collins for information on setting up milvus and text embedding\n",
    "# https://dev.to/stephenc222/how-to-use-milvus-to-store-and-query-vector-embeddings-5hhl\n",
    "from pymilvus import connections\n",
    "\n",
    "def connect_to_milvus():\n",
    "    try:\n",
    "        connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "        print(\"Connected to Milvus.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Milvus: {e}\")\n",
    "        raise\n",
    "\n",
    "connect_to_milvus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c52ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up schema and create a collection\n",
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "def create_collection(name, fields, description):\n",
    "    schema = CollectionSchema(fields, description)\n",
    "    collection = Collection(name, schema, consistency_level=\"Strong\")\n",
    "    return collection\n",
    "\n",
    "def drop_collection(name):\n",
    "    collection = Collection(name)\n",
    "    collection.drop()\n",
    "    \n",
    "# Define fields for our collection\n",
    "fields = [\n",
    "    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n",
    "    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=500),\n",
    "]\n",
    "\n",
    "drop_collection(\"wikipedia_simple\")\n",
    "collection = create_collection(\"wikipedia_simple\", fields, \"Text embeddings of the simple wikipedia dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb65455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_util import generate_embeddings\n",
    "# Generate embeddings for each article\n",
    "for i, doc in enumerate(wikipedia['text']):\n",
    "    embedding = generate_embeddings(doc)\n",
    "    # Write into file\n",
    "    with open(\"embeddings.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(f\"{embedding}\\n\")\n",
    "    print(f\"{i}/{len(wikipedia)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embeddings\n",
    "with open(\"embeddings.txt\", \"r\", encoding='utf-8') as f:\n",
    "    embeddings = f.readlines()\n",
    "\n",
    "embeddings = [[float(value) for value in embedding[1:-2].split(\", \")] for embedding in embeddings]\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into milvus\n",
    "entities = [\n",
    "    [str(i) for i in range(len(wikipedia))],\n",
    "    embeddings,\n",
    "    [str(title) for title in wikipedia['title']],\n",
    "]\n",
    "\n",
    "insert_result = collection.insert(entities)\n",
    "print(insert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for embeddings\n",
    "def create_index(collection, field_name, index_type, metric_type, params):\n",
    "    index = {\"index_type\": index_type, \"metric_type\": metric_type, \"params\": params}\n",
    "    collection.create_index(field_name, index)\n",
    "\n",
    "create_index(collection, \"embeddings\", \"IVF_FLAT\", \"L2\", {\"nlist\": 128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca80661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_query(collection, search_vectors, search_field, search_params):\n",
    "    collection.load()\n",
    "    result = collection.search(search_vectors, search_field, search_params, limit=3, output_fields=[\"title\"])\n",
    "    return result[0][0].entity.get(\"title\")\n",
    "\n",
    "# Test search\n",
    "query = \"how do living organisms in a natural environment respond to changes in weather or climate?\"\n",
    "query_vector = generate_embeddings(query)\n",
    "search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}})\n",
    "\n",
    "# Correctly returns \"Environment\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of our model\n",
    "score = 0\n",
    "totalScore = 0\n",
    "\n",
    "for set in train[:500]:\n",
    "    query = set['question']\n",
    "    query_vector = generate_embeddings(query)\n",
    "    result = search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}})\n",
    "    print(f\"result: {result}, answer: {set['article']}\")\n",
    "    if result.lower() in set['article'].lower():\n",
    "        score += set['points']\n",
    "    totalScore += set['points']\n",
    "\n",
    "print(f\"Our model scored {score}/{totalScore} points on the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model scored 15618/31274 points on the training set. \n",
    "# Let's see if we can improve this by weighing based on the points and whether the answer is in the question.\n",
    "\n",
    "# Add a is_in_question field to the train set\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(train)\n",
    "\n",
    "# Create the 'is_in_question' column\n",
    "df['is_in_question'] = df.apply(lambda row: row['article'].lower() in row['question'].lower(), axis=1).astype(int)\n",
    "\n",
    "X = df[['points']]\n",
    "y = df['is_in_question']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f\"Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the search function to take into account the points and whether the answer is in the question\n",
    "def search_and_query(collection, search_vectors, search_field, search_params, points, question):\n",
    "    collection.load()\n",
    "    result = collection.search(search_vectors, search_field, search_params, limit=3, output_fields=[\"title\"])\n",
    "    in_question_prob = model.predict_proba([[points]])[0][1]\n",
    "\n",
    "    processed_results = [[article.entity.get(\"title\"), 1 - article.distance] for article in result[0]]\n",
    "\n",
    "    for article in processed_results:\n",
    "        if article[0].lower() in question.lower():\n",
    "            article[1] *= in_question_prob\n",
    "        else:\n",
    "            article[1] *= (1-in_question_prob)\n",
    "    \n",
    "    processed_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return processed_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of our new model\n",
    "score = 0\n",
    "totalScore = 0\n",
    "\n",
    "for set in train[:500]:\n",
    "    query = set['question']\n",
    "    query_vector = generate_embeddings(query)\n",
    "    result = search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}, set['points'], set['question'])\n",
    "    print(f\"result: {result}, answer: {set['article']}\")\n",
    "    if result.lower() in set['article'].lower():\n",
    "        score += set['points']\n",
    "    totalScore += set['points']\n",
    "\n",
    "print(f\"Our model scored {score}/{totalScore} points on the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model scored 7402/31274 points on the training set.\n",
    "# Whoops, it's much worse. Let's stick with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to the old model\n",
    "\n",
    "def search_and_query(collection, search_vectors, search_field, search_params):\n",
    "    collection.load()\n",
    "    result = collection.search(search_vectors, search_field, search_params, limit=3, output_fields=[\"title\"])\n",
    "    return result[0][0].entity.get(\"title\")\n",
    "\n",
    "# Read the test set\n",
    "\n",
    "test = []\n",
    "with open(\"test.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        test.append(json.loads(line))\n",
    "\n",
    "print(test[0]['question'], test[0]['points'])\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07a85a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results for the test set\n",
    "for set in test:\n",
    "    query = set['question']\n",
    "    query_vector = generate_embeddings(query)\n",
    "    result = search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}})\n",
    "    set['answer'] = result\n",
    "\n",
    "    with open(\"submission.jsonl\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(f\"{set}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f08527",
   "metadata": {},
   "source": [
    "# Your algorithm choice\n",
    "I used a vector database to store the embeddings of each article with a label of the article title. I then generated embeddings of each question and did a vector similarity search to find the most similar articles.\n",
    "\n",
    "# How you would extend this algorithm to 100k/1m/1b articles\n",
    "This method should still be workable for 100k articles, but the database might be too large for 1m/1b articles. I investigated the proportion of nouns at the start because I thought it may be a good idea to do a breadth-first search by looking up the articles for all the nouns in the question, and then looking for the answer in these articles. If those articles don't contain a satisfactory answer, we could then extract the nouns from these articles and search until we find an answer. For the small size of data I was provided this wasn't necessary, this method could work to ensure that only a small number of articles are added to the vector database and we don't run out of space/time.\n",
    "\n",
    "# Evaluating performance\n",
    "Since I didn't have to do training, the entire train dataset acted as my train dataset. \n",
    "I don't have much experience working with docker and my Milvus container kept stopping on its own (possibly due to memory issues as I'm running this on my laptop). Because of this, I had to limit the sample size to the first 500 entries but I think this still gave a pretty good evaluation of model performance especially as I was comparing the version which weighted based on points.\n",
    "\n",
    "# Any reference that you found interesting\n",
    "I've worked with vector databases before with Pinecone but it was my first time setting it up and running it on my laptop with Milvus. It was a good learning process!\n",
    "\n",
    "# Ideas that worked / did not work \n",
    "As discussed, open source language embedding models are advanced enough to make my more primitive methods relying on NLP to be obsolete at this scale.\n",
    "\n",
    "I was surprised my trick of using the points to observe whether the answer is in the title caused the performance to drop so drastically, but this could be because the relationship between points and the answer being in the question wasn't strong in the first place. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
